<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="robots" content="noindex, nofollow">
    <title>Call for Papers - 3D Geometry Generation for Scientific Computing</title>
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet">
    <link href="./style.css" rel="stylesheet" type="text/css" media="all" />
</head>
<body>
    <div id="header"></div>
    <div id="navbar"></div>

    <div class="container mt-5">
        <section id="call_for_papers">
            <h2>Call for Papers</h2>
            <p>We invite paper submissions to the workshop on 3D Geometry Generation for Scientific Computing (3D4S) co-located with the NeurIPS 2025, the International Conference on Machine Learning, at Vancouver, BC, Canada.</p>

            <section id="topics" class="Topics">
                <h3>Topics will include, but are not limited to:</h3>
                <ul>
                    <li>Multi-scale Patterns: How can 3D/4D models effectively capture both fine-grained and large-scale details in complex scientific datasets, such as fluid and smoke?</li>
                    <li>Large-scale Scenes: What techniques can improve the scalability of 3D/4D reconstructions for large environments like cities, forests, or glaciers, without sacrificing accuracy or computational feasibility?</li>
                    <li>Heterogeneous Views: How can we effectively integrate data from multiple sources (e.g., satellite, LiDAR, drone, mobile devices) to produce accurate and seamless 3D models while minimizing noise and alignment issues?</li>
                    <li>Dynamic and Time-varying Views: What methods can improve temporal coherence in 4D reconstructions of dynamic scenes, such as fast-moving natural systems or urban traffic, while avoiding artifacts?</li>
                    <li>Complex and Unstructured Geometries: How can 3D/4D models better handle irregular, unstructured geometries found in natural environments like mountains or coral reefs, particularly in the presence of sharp features?</li>
                    <li>Occlusions and Missing Observations: What techniques can be developed to fill gaps in occluded or incomplete data in real-world scenarios, ensuring accurate reconstructions despite missing perspectives or environmental obstacles?</li>
                    <li>Computational Complexity: How can we reduce the computational cost of high-quality 3D/4D reconstructions, especially for real-time or large-scale applications that require high-resolution output?</li>
                    <li>Generalization and Scene Adaptability: What approaches can help 3D/4D models generalize to new environments without retraining, enabling wider applicability across different scientific domains?</li>
                    <li>Real-time Rendering for Dynamic Scenes: How can we achieve real-time rendering for dynamic 4D scenes in complex environments, such as simulating natural disasters or fast-moving ecosystems?</li>
                    <li>Lighting and Viewpoint Variations: What novel algorithms can improve the robustness of 3D reconstructions in variable lighting or challenging viewpoints (e.g., low-light conditions or extreme weather)?</li>
                </ul>
                <p><strong>Scientific Domains.</strong> We invite paper submissions from various scientific domains, including but not limited to: Fluid Dynamics, Climate and Glaciology, Biomedicine and Medical Research, Astronomy and Planetary Science, Material Science, Physics and High Energy Research, Astrophysics and Space Science, Computational Modeling and Forecasting, Earth Science, Chemistry and Small Molecules, Ecology and Environmental Studies, Geosciences and Geology, Urban Planning and Architecture. Applications-driven submissions focusing on 3D/4D reconstructions for scientific data are also highly encouraged.</p>
            </section>

            <h3>Tentative important dates (AoE time):</h3>
            <ul>
                <!-- <li>Abstract Submission Deadline: March 1, 2025</li> -->
                <li>Abstract Submission Deadline: May 13, 2025</li> <!-- after NeurIPS ddl -->
                <!-- <li>Paper Submission Deadline: March 4, 2025</li> -->
                <li>Paper Submission Deadline: May 16, 2025</li>
                <!-- <li>Review Bidding Period: March 4 - March 8, 2025</li> -->
                <li>Review Bidding Period: May 16 - May 20, 2025</li>
                <!-- <li>Review Deadline: March 25, 2025</li> -->
                <li>Review Deadline: June 7, 2025</li>
                <!-- <li>Acceptance/Rejection Notification Date: March 26, 2025</li> -->
                <li>Acceptance/Rejection Notification Date: June 9, 2025</li>
                <!-- <li>Camera-Ready Submission: April 5, 2025</li> -->
                <li>Camera-Ready Submission: June 19, 2025</li>
                <!-- <li>Workshop Date: June 11 or 12, 2025</li> -->
                <li>Workshop Date: July 18 or 19, 2025</li>
            </ul>

            <h3>Awards</h3>
            <p>Among exceptional research papers with high review scores, we will select one best paper award and two runner-ups.</p>
            
            <h3>Submission policy</h3>
            <p>Submission Format (inherit from NeurIPS 2025): <a href="https://media.icml.cc/Conferences/ICML2025/Styles/icml2025.zip" target="_blank"><b>Latex Template</b></a>.</p>
            <p>Submission portal: <a href="https://openreview.net/group?id=NeurIPS.cc/2025/Workshop/3D4S" target="_blank"><b>OpenReview</b></a>.</p>
            <p>
                Our submission policy is inherited from <a href="https://icml.cc/Conferences/2025/CallForPapers" target="_blank">NeurIPS 2025</a>.
                <!-- All authors and submissions must adhere to the <a href="https://cvpr.thecvf.com/Conferences/2025/EthicsGuidelines" target="_blank">NeurIPS 2025 Ethics Guidelines for Authors</a>. -->
            </p>
            <ul>
                <li>
                    <b>Submissions</b>: The main text of a submitted paper must be <b>at least four pages long and no more than eight pages</b>, including all figures and tables. Additional pages containing references don’t count as content pages.
                    <!-- If your submission is accepted, you will be allowed an additional content page for the camera-ready version. -->
                <ul>
                    <li>The main text and references may be followed by technical appendices, for which there is no page limit.</li>
                    <li>The maximum file size for a full submission, which includes technical appendices, is 50MB.</li>
                </ul></li>
                <li>
                    <b>Supplementary material</b>: While all technical appendices should be included as part of the main paper submission PDF, authors may submit up to 100MB of supplementary material, such as data, or source code in a ZIP format.
                    Supplementary material should be material created by the authors that directly supports the submission content. Like submissions, supplementary material must be anonymized. Looking at supplementary material is at the discretion of the reviewers.
                <ul>
                    <li>
                        We encourage authors to upload their code and data as part of their supplementary material in order to help reviewers assess the quality of the work.
                        Check <a href="https://icml.cc/Conferences/2025/AuthorInstructions" target="_blank">NeurIPS 2025 Author Instructions</a>, as well as code submission <a href="https://github.com/paperswithcode/releasing-research-code" target="_blank">guidelines and templates</a> for further details.</li>
                </ul></li>
                <li><b>Use of Large Language Models (LLMs)</b>: We welcome authors to use any tool that is suitable for preparing high-quality papers and research. However, we ask authors to keep in mind two important criteria. First, we expect papers to fully describe their methodology, and any tool that is important to that methodology, including the use of LLMs, should be described also. For example, authors should mention tools (including LLMs) that were used for data processing or filtering, visualization, facilitating or running experiments, and proving theorems. It may also be advisable to describe the use of LLMs in implementing the method (if this corresponds to an important, original, or non-standard component of the approach). Second, authors are responsible for the entire content of the paper, including all text and figures, so while authors are welcome to use any tool they wish for writing the paper, they must ensure that all text is correct and original.</li>
                <li><b>Double-blind reviewing</b>:  All submissions must be anonymized and may not contain any identifying information that may violate the double-blind reviewing policy.  This policy applies to any supplementary or linked material as well, including code.  If you are including links to any external material, it is your responsibility to guarantee anonymous browsing.  Please do not include acknowledgements at submission time. If you need to cite one of your own papers, you should do so with adequate anonymization to preserve double-blind reviewing.  For instance, write “In the previous work of Smith et al. [1]...” rather than “In our previous work [1]...”). If you need to cite one of your own papers that is in submission to NeurIPS and not available as a non-anonymous preprint, then include a copy of the cited anonymized submission in the supplementary material and write “Anonymous et al. [1] concurrently show...”). Any papers found to be violating this policy will be rejected.</li>
                <li><b>Abstract Submission</b>: There is a mandatory abstract submission deadline on <b>May 13, 2025 (AoE time)</b>, three days before full paper submissions are due. While it will be possible to edit the title and abstract until the full paper submission deadline, submissions with "placeholder" abstracts that are rewritten for the full submission risk being removed without consideration. This includes titles and abstracts that either provide little or no semantic information (e.g., "We provide a new semi-supervised learning method.") or describe a substantively different claimed contribution.  The author list cannot be changed after the abstract deadline. After that, authors may be reordered, but any additions or removals must be justified in writing and approved on a case-by-case basis by the program chairs only in exceptional circumstances. </li>
                <li><b>Anti-collusion</b>: NeurIPS does not tolerate any collusion whereby authors secretly cooperate with reviewers, ACs or SACs to obtain favorable reviews. </li>
                <li><b>Publication of accepted submissions</b>:  Reviews, meta-reviews, and any discussion with the authors will be made public for accepted papers (but reviewer, area chair, and senior area chair identities will remain anonymous). Camera-ready papers will be due in advance of the conference. All camera-ready papers must include a funding disclosure. We strongly encourage accompanying code and data to be submitted with accepted papers when appropriate, as per the code submission policy. Authors will be allowed to make minor changes for a short period of time after the conference.</li>
                <li>Posting papers on preprint servers like ArXiv is permitted.</li>
                <li>All submissions must represent original work and not previously published elsewhere.</li>
                <li>Submissions are only accepted in written English.</li>
                <li>All papers must be proofread by the authors before submission.</li>
                <li>This workshop is non-archival; even though all accepted papers will be available on OpenReview, there are no formally-published proceedings.</li>
            </ul>

            <h3>Contact</h3>
            <p>If you have any questions about paper submission and the workshop, please send email to: <a style="color:blue" href="mailto:3d4sworkshop@gmail.com">3d4sworkshop@gmail.com</a>.</p>
        </section>
    </div>

    <button id="goToTopBtn" title="Go to top">Top</button>

    <div id="footer"></div>

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.9.3/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
    <script src="./custom.js"></script>
    <script src="./load-templates.js"></script>
</body>
</html>
